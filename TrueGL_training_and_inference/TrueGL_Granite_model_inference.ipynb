{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ee23347",
   "metadata": {},
   "source": [
    "### Evaluate the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2187d58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm # For progress bars\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re # For improved label extraction\n",
    "import numpy as np # For AE calculation\n",
    "\n",
    "# === CONFIG (User should verify these paths and settings) ===\n",
    "# Model for Reliability Scoring (Fine-tuned)\n",
    "CHECKPOINTS_DIR = \"/root/Fine-Tuning_Truth/granite-finetuned-articles\" # Path to directory of fine-tuned checkpoints\n",
    "\n",
    "# Model for Justification (Base Model)\n",
    "BASE_MODEL_PATH = \"/root/Fine-Tuning_Truth/granite-3.1-1b-a400m-base\" # Path to the original base model\n",
    "\n",
    "CSV_PATH = \"val_articles_fine_tuning.csv\"  # Update with your new data path\n",
    "BATCH_SIZE_PREDICTION = 20\n",
    "BATCH_SIZE_JUSTIFICATION = 20 # May need to be smaller for base model if it's larger or for longer prompts\n",
    "MAX_LENGTH_PREDICTION = 1660\n",
    "# Increase MAX_LENGTH_JUSTIFICATION for the few-shot prompt.\n",
    "# Base models might handle longer contexts better if this is increased.\n",
    "MAX_LENGTH_JUSTIFICATION = 768 # Max length for tokenizing justification prompts (includes statement & few-shot examples)\n",
    "MAX_NEW_TOKENS_PREDICTION = 10\n",
    "MAX_NEW_TOKENS_JUSTIFICATION = 300 # Desired length for the justification text\n",
    "\n",
    "USE_GPUS = list(range(8))\n",
    "\n",
    "# === Set visible GPUs ===\n",
    "if USE_GPUS:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, USE_GPUS))\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Using GPUs: {os.environ['CUDA_VISIBLE_DEVICES']}\")\n",
    "        DEVICE = \"cuda\"\n",
    "    else:\n",
    "        print(\"CUDA not available, using CPU.\")\n",
    "        DEVICE = \"cpu\"\n",
    "else:\n",
    "    print(\"No GPUs specified, using CPU.\")\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "# === Helper to find latest checkpoint ===\n",
    "def get_latest_checkpoint(checkpoints_dir):\n",
    "    if not os.path.exists(checkpoints_dir):\n",
    "        print(f\"Error: Checkpoints directory not found at {checkpoints_dir}\")\n",
    "        return None\n",
    "    checkpoints = [d for d in os.listdir(checkpoints_dir) if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(checkpoints_dir, d))]\n",
    "    if not checkpoints:\n",
    "        print(f\"No checkpoints found in {checkpoints_dir}\")\n",
    "        return None\n",
    "    checkpoints.sort(key=lambda x: int(x.split(\"-\")[1]))\n",
    "    latest_checkpoint_name = checkpoints[-1]\n",
    "    print(f\"Found latest checkpoint for scoring: {latest_checkpoint_name}\")\n",
    "    return os.path.join(checkpoints_dir, latest_checkpoint_name)\n",
    "\n",
    "# === Custom Dataset for Reliability Prediction ===\n",
    "class PredictionDataset(Dataset):\n",
    "    def __init__(self, statements, tokenizer, max_length):\n",
    "        self.statements = statements\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.statements)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        statement = str(self.statements[idx])\n",
    "        prompt = f\"Assess the reliability of this statement (article from the Internet) on the scale from 0 to 1, where 0 is completely unreliable and 1 is completely reliable. Do not provide any explanation. Just the number:\\nStatement: {statement}\\nLabel:\"\n",
    "        encoding = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "# === Custom Dataset for Justification Generation (Using Few-Shot) ===\n",
    "class JustificationDataset(Dataset):\n",
    "    def __init__(self, statements, scores, tokenizer, max_length):\n",
    "        self.statements = statements\n",
    "        self.scores = scores\n",
    "        self.tokenizer = tokenizer # This will be the tokenizer for the BASE model\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.statements)\n",
    "\n",
    "    def get_justification_prompt(self, statement, score):\n",
    "        statement_str = str(statement)\n",
    "        max_statement_chars_in_prompt = 700\n",
    "        if len(statement_str) > max_statement_chars_in_prompt:\n",
    "            statement_str = statement_str[:max_statement_chars_in_prompt] + \"...\"\n",
    "\n",
    "        example_statement_reliable = \"The Eiffel Tower is located in Paris, France. It is a famous landmark.\"\n",
    "        example_score_reliable = 0.95\n",
    "        example_justification_reliable = \"\"\"- The statement is factually accurate (Eiffel Tower is in Paris).\n",
    "- It describes a well-known fact, easily verifiable.\n",
    "- Writing quality is good and consistent.\"\"\"\n",
    "\n",
    "        example_statement_unreliable = \"The moon is made of green cheese and visited by cows weekly.\"\n",
    "        example_score_unreliable = 0.05\n",
    "        example_justification_unreliable = \"\"\"- The statement contains obvious factual inaccuracies (moon not cheese, cows don't visit).\n",
    "- It presents scientifically implausible claims.\n",
    "- Lacks any supporting evidence or credibility.\"\"\"\n",
    "\n",
    "        prompt_intro = f\"\"\"You are an expert analyst. Your task is to provide a concise, bullet-point justification for a given reliability score of a statement.\n",
    "The reliability score is on a scale from 0 (completely unreliable) to 1 (completely reliable).\n",
    "Your justification should ONLY be the bullet points explaining the score. Do NOT repeat the statement or the score in your response.\n",
    "\n",
    "Here are some examples of how to format your justification:\n",
    "\n",
    "Example 1:\n",
    "Statement context: \"{example_statement_reliable}\"\n",
    "Assigned reliability score: {example_score_reliable:.2f}\n",
    "Correct Justification:\n",
    "{example_justification_reliable}\n",
    "\n",
    "Example 2:\n",
    "Statement context: \"{example_statement_unreliable}\"\n",
    "Assigned reliability score: {example_score_unreliable:.2f}\n",
    "Correct Justification:\n",
    "{example_justification_unreliable}\n",
    "\n",
    "---\n",
    "Now, provide the justification for the following:\n",
    "\"\"\"\n",
    "        if score == -1.0 or pd.isna(score):\n",
    "            current_task_prompt = f\"\"\"Statement context: \"{statement_str}\"\n",
    "The reliability score for this statement could not be determined.\n",
    "Provide your overall impression of this statement's potential reliability using concise bullet points.\n",
    "Consider factors like text consistency, apparent factual accuracy, and potential AI generation.\n",
    "Justification:\"\"\"\n",
    "        else:\n",
    "            current_task_prompt = f\"\"\"Statement context: \"{statement_str}\"\n",
    "Assigned reliability score: {score:.2f}\n",
    "Provide a concise bullet-point justification for THIS SCORE.\n",
    "Justification:\"\"\"\n",
    "        return prompt_intro + \"\\n\" + current_task_prompt\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        statement = self.statements[idx]\n",
    "        score = self.scores[idx]\n",
    "        prompt_text = self.get_justification_prompt(statement, score)\n",
    "        encoding = self.tokenizer(\n",
    "            prompt_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "# === Improved Label Extraction Function ===\n",
    "def extract_label(text):\n",
    "    try:\n",
    "        match = re.search(r\"Label:(.*?)([\\d\\.]+)\", text, re.IGNORECASE | re.DOTALL)\n",
    "        if match:\n",
    "            label_str = match.group(2).strip()\n",
    "            if label_str.endswith('.'): label_str = label_str[:-1]\n",
    "            return float(label_str)\n",
    "        else:\n",
    "            parts = text.split(\"Label:\")\n",
    "            target_part = parts[-1] if len(parts) > 1 else text\n",
    "            numeric_match = re.search(r\"([\\d\\.]+)\", target_part)\n",
    "            if numeric_match:\n",
    "                label_str = numeric_match.group(1).strip()\n",
    "                if label_str.endswith('.'): label_str = label_str[:-1]\n",
    "                return float(label_str)\n",
    "            return None\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0346a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Starting Evaluation Script ---\")\n",
    "\n",
    "# --- Load Fine-tuned Model for Reliability Scoring ---\n",
    "latest_checkpoint_scoring = get_latest_checkpoint(CHECKPOINTS_DIR)\n",
    "if not latest_checkpoint_scoring:\n",
    "    raise ValueError(\"Exiting due to missing fine-tuned model checkpoint for scoring.\")\n",
    "\n",
    "print(f\"Loading fine-tuned scoring model and its tokenizer from: {latest_checkpoint_scoring}\")\n",
    "try:\n",
    "    tokenizer_scoring = AutoTokenizer.from_pretrained(latest_checkpoint_scoring)\n",
    "    if tokenizer_scoring.pad_token is None:\n",
    "        tokenizer_scoring.pad_token = tokenizer_scoring.eos_token\n",
    "        print(\"Scoring tokenizer pad_token set to eos_token.\")\n",
    "\n",
    "    model_scoring = AutoModelForCausalLM.from_pretrained(\n",
    "        latest_checkpoint_scoring,\n",
    "        torch_dtype=torch.bfloat16 if DEVICE == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float16,\n",
    "        device_map=\"auto\" # Let Hugging Face handle device mapping\n",
    "    )\n",
    "    print(\"Fine-tuned scoring model and tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading fine-tuned scoring model or tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Load Base Model for Justification ---\n",
    "if not os.path.exists(BASE_MODEL_PATH):\n",
    "    raise ValueError(f\"Base model for justification not found at {BASE_MODEL_PATH}\")\n",
    "\n",
    "print(f\"Loading base model for justification and its tokenizer from: {BASE_MODEL_PATH}\")\n",
    "try:\n",
    "    # It's good practice to load the specific tokenizer for the base model,\n",
    "    # even if it's expected to be the same as the fine-tuned one.\n",
    "    tokenizer_justification = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "    if tokenizer_justification.pad_token is None:\n",
    "        tokenizer_justification.pad_token = tokenizer_justification.eos_token\n",
    "        print(\"Justification tokenizer pad_token set to eos_token.\")\n",
    "\n",
    "    model_justification = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_PATH,\n",
    "        torch_dtype=torch.bfloat16 if DEVICE == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float16,\n",
    "        device_map=\"auto\" # Let Hugging Face handle device mapping\n",
    "    )\n",
    "    print(\"Base justification model and tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading base justification model or tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "# === Load data ===\n",
    "try:\n",
    "    df_full = pd.read_csv(CSV_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Data CSV not found at {CSV_PATH}\")\n",
    "    raise\n",
    "if 'statement' not in df_full.columns or 'labels' not in df_full.columns:\n",
    "    print(f\"Error: CSV must contain 'statement' and 'labels' columns. Found: {df_full.columns}\")\n",
    "    raise\n",
    "\n",
    "df = df_full.sample(n=100, random_state=42).reset_index(drop=True)\n",
    "# df = df_full.copy() # For full evaluation\n",
    "print(f\"Loaded data with {len(df)} rows.\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b74cf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Reliability Prediction (using Fine-tuned Model) ---\n",
    "print(\"\\n--- Generating Reliability Predictions ---\")\n",
    "statements_for_prediction = df['statement'].tolist()\n",
    "# Use tokenizer_scoring for the prediction dataset\n",
    "prediction_dataset = PredictionDataset(statements_for_prediction, tokenizer_scoring, MAX_LENGTH_PREDICTION)\n",
    "prediction_dataloader = DataLoader(prediction_dataset, batch_size=BATCH_SIZE_PREDICTION, shuffle=False)\n",
    "\n",
    "predicted_scores_raw = []\n",
    "model_scoring.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(prediction_dataloader, desc=\"Predicting Reliability\", unit=\"batch\"):\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[\"input_ids\"].to(model_scoring.device), # Ensure tensors go to the correct model's device\n",
    "            \"attention_mask\": batch[\"attention_mask\"].to(model_scoring.device)\n",
    "        }\n",
    "        outputs = model_scoring.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS_PREDICTION,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer_scoring.pad_token_id\n",
    "        )\n",
    "        # Use tokenizer_scoring to decode\n",
    "        decoded_outputs = tokenizer_scoring.batch_decode(outputs, skip_special_tokens=True)\n",
    "        for text in decoded_outputs:\n",
    "            predicted_scores_raw.append(text)\n",
    "\n",
    "extracted_labels = []\n",
    "problematic_extractions_indices = []\n",
    "for i, text in enumerate(predicted_scores_raw):\n",
    "    label = extract_label(text)\n",
    "    if label is None:\n",
    "        print(f\"Warning: Could not extract a valid numeric label from raw output for statement index {i}: '{text}'\")\n",
    "        problematic_extractions_indices.append(i)\n",
    "    extracted_labels.append(label)\n",
    "\n",
    "#df['predicted_label_raw'] = predicted_scores_raw\n",
    "df['predicted_label'] = extracted_labels\n",
    "df['predicted_label'] = pd.to_numeric(df['predicted_label'], errors='coerce').fillna(-1.0).astype(float)\n",
    "df['predicted_label'] = df['predicted_label'].apply(lambda x: min(max(x, 0.0), 1.0) if x != -1.0 else -1.0)\n",
    "\n",
    "num_failed_predictions = (df['predicted_label'] == -1.0).sum()\n",
    "print(f\"Number of statements where label extraction failed (marked as -1.0): {num_failed_predictions} out of {len(df)}\")\n",
    "if problematic_extractions_indices:\n",
    "    print(f\"Indices of statements with problematic extractions: {problematic_extractions_indices[:20]} (showing first 20 if many)\")\n",
    "\n",
    "df1 = df[df['predicted_label'] != -1.0].copy()\n",
    "if not df1.empty:\n",
    "    df1['AE'] = np.abs(df1['predicted_label'] - df1['labels'])\n",
    "    df1['AE'] = df1['AE'].round(3)\n",
    "    print(\"\\nAbsolute Error (AE) calculated for valid predictions:\")\n",
    "    print(df1[['statement', 'labels', 'predicted_label', 'AE']].head())\n",
    "    print(\"\\nValue counts for AE (on valid predictions):\")\n",
    "    print(df1['AE'].value_counts().sort_index())\n",
    "    mae = df1['AE'].mean()\n",
    "    print(f\"\\nMean Absolute Error (MAE) on valid predictions: {mae:.4f}\")\n",
    "else:\n",
    "    print(\"No valid predictions to calculate AE.\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ba0ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Justification Generation (using Base Model) ---\n",
    "print(\"\\n--- Generating Justifications ---\")\n",
    "\n",
    "statements_for_justification = df['statement'].tolist()\n",
    "scores_for_justification = df['predicted_label'].tolist()\n",
    "\n",
    "# Use tokenizer_justification for the justification dataset\n",
    "justification_dataset = JustificationDataset(statements_for_justification, scores_for_justification, tokenizer_justification, MAX_LENGTH_JUSTIFICATION)\n",
    "justification_dataloader = DataLoader(justification_dataset, batch_size=BATCH_SIZE_JUSTIFICATION, shuffle=False)\n",
    "\n",
    "generated_justifications_clean = []\n",
    "model_justification.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(justification_dataloader, desc=\"Generating Justifications\", unit=\"batch\"):\n",
    "        input_ids_batch = batch[\"input_ids\"].to(model_justification.device) # Ensure tensors go to correct model's device\n",
    "        attention_mask_batch = batch[\"attention_mask\"].to(model_justification.device)\n",
    "\n",
    "        outputs = model_justification.generate(\n",
    "            input_ids=input_ids_batch,\n",
    "            attention_mask=attention_mask_batch,\n",
    "            max_new_tokens=MAX_NEW_TOKENS_JUSTIFICATION,\n",
    "            do_sample=True,\n",
    "            temperature=0.6, # Base models might benefit from slightly higher temp if too bland, or lower if too random\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2, # Still useful for base models\n",
    "            pad_token_id=tokenizer_justification.pad_token_id\n",
    "        )\n",
    "        \n",
    "        for i in range(outputs.shape[0]):\n",
    "            prompt_tokens_count = input_ids_batch.shape[1]\n",
    "            justification_tokens = outputs[i][prompt_tokens_count:]\n",
    "            # Use tokenizer_justification to decode\n",
    "            justification_text = tokenizer_justification.decode(justification_tokens, skip_special_tokens=True).strip()\n",
    "            \n",
    "            # Basic cleanup if model still prepends \"Justification:\"\n",
    "            if justification_text.lower().startswith(\"justification:\"):\n",
    "                justification_text = justification_text[len(\"justification:\"):].strip()\n",
    "            # Additional cleanup: remove any repeated prompt fragments if the model is still confused by \"Statement context:\"\n",
    "            # This is a bit of a heuristic.\n",
    "            if \"Statement context:\".lower() in justification_text.lower() :\n",
    "                 # Try to find the text *after* the last occurrence of \"Statement context:\" or the actual justification markers\n",
    "                parts_after_context = re.split(r'Correct Justification:|Justification:', justification_text, flags=re.IGNORECASE)\n",
    "                if len(parts_after_context) >1:\n",
    "                    justification_text = parts_after_context[-1].strip()\n",
    "\n",
    "\n",
    "            generated_justifications_clean.append(justification_text)\n",
    "\n",
    "df['predicted_justification'] = generated_justifications_clean\n",
    "print(\"\\n--- Sample of Statements with Predictions and Cleaned Justifications ---\")\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "print(df[['statement', 'labels', 'predicted_label', 'predicted_justification']].head())\n",
    "\n",
    "if len(df['predicted_justification']) > 0 and df['predicted_justification'][0]:\n",
    "    print(\"\\nSample justification (first item):\")\n",
    "    print(df['predicted_justification'][0])\n",
    "    print(f\"Length of first justification: {len(tokenizer_justification.encode(df['predicted_justification'][0]))} tokens (approx)\")\n",
    "else:\n",
    "    print(\"\\nFirst justification is empty or not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafdcd68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Alex_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
