{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d962ea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm # For progress bars\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re # For improved label extraction\n",
    "import numpy as np # For AE calculation\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73dc0ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "example1_score = 0.1\n",
    "example2_score = 0.35\n",
    "example3_score = 1.0\n",
    "\n",
    "example1_statement = 'Dogs can photosynthesize sunlight to produce their own food.'\n",
    "\n",
    "example2_statement = 'Itâ€™s often said that sleeping less than four hours per night improves creativity.'\n",
    "\n",
    "example3_statement = 'Mount Everest is the highest mountain above sea level on Earth.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9fee13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_prompt(statement: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates a concise prompt for a raw LLM to predict a reliability score\n",
    "    on a scale of 0.1 to 1.0 based on the statement's content, using few-shot examples.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Assess the reliability of the following statement and provide a numerical score between 0.1 (completely unreliable) and 1.00 (perfectly reliable). Aim for a precise score.\n",
    "Here are a few examples of statements and their corresponding reliability scores:\n",
    "Example 1:\n",
    "Statement: {example1_statement}\n",
    "Score: {example1_score:.2f}\n",
    "Example 2:\n",
    "Statement: {example2_statement}\n",
    "Score: {example2_score:.2f}\n",
    "Example 3:\n",
    "Statement: {example3_statement}\n",
    "Score: {example3_score:.2f}\n",
    "Now, assess the reliability of the following statement according to the instructions and examples above:\n",
    "Statement: {statement}\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a93d45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPUs: 0,1,2,3,4,5,6,7,8,9\n"
     ]
    }
   ],
   "source": [
    "CSV_PATH = \"val_short_granite.csv\"  # Update with your new data path\n",
    "BATCH_SIZE_PREDICTION = 15\n",
    "MAX_LENGTH_PREDICTION = 2240\n",
    "# Increase MAX_LENGTH_JUSTIFICATION for the few-shot prompt.\n",
    "# Base models might handle longer contexts better if this is increased.\n",
    "MAX_NEW_TOKENS_PREDICTION = 20 # 10\n",
    "\n",
    "USE_GPUS = list(range(10))\n",
    "\n",
    "# === Set visible GPUs ===\n",
    "if USE_GPUS:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, USE_GPUS))\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Using GPUs: {os.environ['CUDA_VISIBLE_DEVICES']}\")\n",
    "        DEVICE = \"cuda\"\n",
    "    else:\n",
    "        print(\"CUDA not available, using CPU.\")\n",
    "        DEVICE = \"cpu\"\n",
    "else:\n",
    "    print(\"No GPUs specified, using CPU.\")\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "class PredictionDataset(Dataset):\n",
    "    def __init__(self, statements, tokenizer, max_length):\n",
    "        self.statements = statements\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.statements)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        statement = str(self.statements[idx])\n",
    "        # Compose the initial prompt up to the \"Reliability Score:\" part\n",
    "        # prompt_prefix = (\n",
    "        #     \"Assess the reliability score of the following statement on a continuous scale from 0.1 (very unreliable) to 1 (perfectly reliable). \"\n",
    "        #     \"You should consider factors such as factual accuracy, verifiability, logical coherence, and evidence transparency. Provide only the numerical score:\\n\"\n",
    "        #     f\"Statement: {statement}\\n\"\n",
    "        # )\n",
    "\n",
    "        prompt_prefix = get_prediction_prompt(statement)\n",
    "\n",
    "        # Reserve 10 tokens for \"Reliability Score:\"\n",
    "        max_prompt_tokens = self.max_length - 10\n",
    "        # Tokenize the prefix and truncate if necessary\n",
    "        prefix_ids = self.tokenizer.encode(prompt_prefix, add_special_tokens=False)\n",
    "        if len(prefix_ids) > max_prompt_tokens:\n",
    "            # Truncate the prefix to fit\n",
    "            prefix_ids = prefix_ids[:max_prompt_tokens]\n",
    "            prompt_prefix = self.tokenizer.decode(prefix_ids, skip_special_tokens=True)\n",
    "        # Add the \"Reliability Score:\" at the end\n",
    "        prompt = prompt_prefix + \"Reliability Score:\"\n",
    "        encoding = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "    \n",
    "def extract_label(text):\n",
    "    try:\n",
    "        match = re.search(r\"Reliability Score:(.*?)([\\d\\.]+)\", text, re.IGNORECASE | re.DOTALL)\n",
    "        if match:\n",
    "            label_str = match.group(2).strip()\n",
    "            if label_str.endswith('.'): label_str = label_str[:-1]\n",
    "            return float(label_str)\n",
    "        else:\n",
    "            parts = text.split(\"Reliability Score:\")\n",
    "            target_part = parts[-1] if len(parts) > 1 else text\n",
    "            numeric_match = re.search(r\"([\\d\\.]+)\", target_part)\n",
    "            if numeric_match:\n",
    "                label_str = numeric_match.group(1).strip()\n",
    "                if label_str.endswith('.'): label_str = label_str[:-1]\n",
    "                return float(label_str)\n",
    "            return None\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "189d1dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv(CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbf1350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(CHECKPOINTS_DIR, padding_side='left'):\n",
    "    df = df_full.copy()\n",
    "    print('Use model: ', CHECKPOINTS_DIR)\n",
    "    tokenizer_scoring = AutoTokenizer.from_pretrained(CHECKPOINTS_DIR, padding_side=padding_side)\n",
    "    if tokenizer_scoring.pad_token is None:\n",
    "        tokenizer_scoring.pad_token = tokenizer_scoring.eos_token\n",
    "    model_scoring = AutoModelForCausalLM.from_pretrained(\n",
    "        CHECKPOINTS_DIR,\n",
    "        torch_dtype=torch.bfloat16 if DEVICE == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float16,\n",
    "        device_map=\"auto\" # Let Hugging Face handle device mapping\n",
    "    )\n",
    "    # --- 1. Reliability Prediction (using Fine-tuned Model) ---\n",
    "    statements_for_prediction = df['statement'].tolist()\n",
    "    # Use tokenizer_scoring for the prediction dataset\n",
    "    prediction_dataset = PredictionDataset(statements_for_prediction, tokenizer_scoring, MAX_LENGTH_PREDICTION)\n",
    "    prediction_dataloader = DataLoader(prediction_dataset, batch_size=BATCH_SIZE_PREDICTION, shuffle=False)\n",
    "    predicted_scores_raw = []\n",
    "    model_scoring.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(prediction_dataloader, desc=\"Predicting Reliability\", unit=\"batch\"):\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[\"input_ids\"].to(model_scoring.device), # Ensure tensors go to the correct model's device\n",
    "                \"attention_mask\": batch[\"attention_mask\"].to(model_scoring.device)\n",
    "            }\n",
    "            outputs = model_scoring.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS_PREDICTION,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer_scoring.pad_token_id\n",
    "            )\n",
    "            # Use tokenizer_scoring to decode\n",
    "            decoded_outputs = tokenizer_scoring.batch_decode(outputs, skip_special_tokens=True)\n",
    "            for text in decoded_outputs:\n",
    "                predicted_scores_raw.append(text)\n",
    "\n",
    "    # Extract the part after 'Reliability Score:' for each item in predicted_scores_raw\n",
    "    # predicted_scores_raw = [\n",
    "    #     text.split('Reliability Score:', 1)[1].strip() if 'Reliability Score:' in text else text\n",
    "    #     for text in predicted_scores_raw\n",
    "    # ]\n",
    "    extracted_labels = []\n",
    "    problematic_extractions_indices = []\n",
    "    for i, text in enumerate(predicted_scores_raw):\n",
    "        label = extract_label(text)\n",
    "        if label is None:\n",
    "            print(f\"Warning: Could not extract a valid numeric label from raw output for statement index {i}: '{text}'\")\n",
    "            problematic_extractions_indices.append(i)\n",
    "        extracted_labels.append(label)\n",
    "\n",
    "    df['predicted_label_raw'] = predicted_scores_raw\n",
    "    df['predicted_label'] = extracted_labels\n",
    "    df['predicted_label'] = pd.to_numeric(df['predicted_label'], errors='coerce').fillna(-1.0).astype(float)\n",
    "    df['predicted_label'] = df['predicted_label'].apply(lambda x: min(max(x, 0.0), 1.0) if x != -1.0 else -1.0)\n",
    "\n",
    "    df1 = df[df['predicted_label'] != -1.0].copy()\n",
    "    mae = mean_absolute_error(df1['labels'], df1['predicted_label'])\n",
    "    mse = mean_squared_error(df1['labels'], df1['predicted_label'])\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    r2 = r2_score(df1['labels'], df1['predicted_label'])\n",
    "    st_dev = float(df1['predicted_label'].std())\n",
    "    num_failed_predictions_ratio = (len(df) - len(df1)) / len(df)\n",
    "    majority_prediction_ratio = float(df1.predicted_label.value_counts().max()) / len(df1)\n",
    "    most_common = float(df1.predicted_label.value_counts().keys()[0])\n",
    "    return df1, mae, rmse, r2, st_dev, num_failed_predictions_ratio, majority_prediction_ratio, most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e467512",
   "metadata": {},
   "source": [
    "### Granite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693ba694",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINTS_DIR = \"/root/Fine-Tuning_Truth/granite-3.1-1b-a400m-base\"\n",
    "df_granite, mae_granite, rmse_granite, r2_granite, st_dev_granite, num_failed_predictions_ratio_granite, majority_prediction_ratio_granite, most_common_granite = evaluate_model(CHECKPOINTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95ced247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0,\n",
       " 0.7604630454140695,\n",
       " 0.5,\n",
       " 0.2813143365983972,\n",
       " 0.32455678937290755,\n",
       " -0.08982214832630864,\n",
       " 0.12386311616333191)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_failed_predictions_ratio_granite, majority_prediction_ratio_granite, most_common_granite, mae_granite, rmse_granite, r2_granite, st_dev_granite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d22d3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_filename = 'histograms/granite_raw_histogram_p3.png'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_granite['predicted_label'], bins=12, kde=True, color='blue')\n",
    "plt.title(f'Histogram of Scores for Granite-1B Model (Prompt v3)')\n",
    "plt.xlabel('Reliability Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.savefig(hist_filename)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3065d3c9",
   "metadata": {},
   "source": [
    "### LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e5f938",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINTS_DIR = \"/root/Fine-Tuning_Truth/local_models/TinyLlama-1.1B-Chat-v1.0\"\n",
    "df_llama, mae_llama, rmse_llama, r2_llama, st_dev_llama, num_failed_predictions_ratio_llama, majority_prediction_ratio_llama, most_common_llama = evaluate_model(CHECKPOINTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c66a316d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.05253784505788068,\n",
       " 0.581766917293233,\n",
       " 0.1,\n",
       " 0.2894708646616542,\n",
       " 0.413841808295432,\n",
       " -0.7610839368108193,\n",
       " 0.277327794035006)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_failed_predictions_ratio_llama, majority_prediction_ratio_llama, most_common_llama, mae_llama, rmse_llama, r2_llama, st_dev_llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d058d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_filename = 'histograms/llama_histogram_p3.png'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_llama['predicted_label'], bins=12, kde=True, color='blue')\n",
    "plt.title(f'Histogram of Scores for TinyLlama-1.1B Model (Prompt v3)')\n",
    "plt.xlabel('Reliability Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.savefig(hist_filename)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46215408",
   "metadata": {},
   "source": [
    "### Olmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343f0b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINTS_DIR = \"/root/Fine-Tuning_Truth/local_models/olmo-2-0425-1b-instruct\"\n",
    "df_olmo, mae_olmo, rmse_olmo, r2_olmo, st_dev_olmo, num_failed_predictions_ratio_olmo, majority_prediction_ratio_olmo, most_common_olmo = evaluate_model(CHECKPOINTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87279fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0,\n",
       " 0.3686553873552983,\n",
       " 1.0,\n",
       " 0.4674158504007124,\n",
       " 0.5781094052311733,\n",
       " -2.4577556736853428,\n",
       " 0.2083923604679034)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_failed_predictions_ratio_olmo, majority_prediction_ratio_olmo, most_common_olmo, mae_olmo, rmse_olmo, r2_olmo, st_dev_olmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07f04ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_filename = 'histograms/olmo_histogram_p3.png'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_olmo['predicted_label'], bins=12, kde=True, color='blue')\n",
    "plt.title(f'Histogram of Scores for Olmo-1B Model (Prompt v3)')\n",
    "plt.xlabel('Reliability Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.savefig(hist_filename)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5805e72",
   "metadata": {},
   "source": [
    "### Granite Fine-Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f76e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINTS_DIR = \"/root/Fine-Tuning_Truth/granite-V2-articles\"\n",
    "df_granite_ft, mae_granite_ft, rmse_granite_ft, r2_granite_ft, st_dev_granite_ft, num_failed_predictions_ratio_granite_ft, majority_prediction_ratio_granite_ft, most_common_granite_ft = evaluate_model(CHECKPOINTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71e1638a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0017809439002671415,\n",
       " 0.36128456735057984,\n",
       " 0.15,\n",
       " 0.14088581623550403,\n",
       " 0.20701247554488625,\n",
       " 0.5565352843057769,\n",
       " 0.2194935836808187)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_failed_predictions_ratio_granite_ft, majority_prediction_ratio_granite_ft, most_common_granite_ft, mae_granite_ft, rmse_granite_ft, r2_granite_ft, st_dev_granite_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f6f815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_filename = 'histograms/granite_fine_tuned_histogram_p3.png'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_granite_ft['predicted_label'], bins=12, kde=True, color='green')\n",
    "plt.title(f'Histogram of Scores for TrueGL Model (Prompt v3)')\n",
    "plt.xlabel('Reliability Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.savefig(hist_filename)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da52ca21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Alex_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
